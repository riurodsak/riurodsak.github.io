<!doctype html><html lang=en><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=author content><meta name=description content="Cross-posted from Cybercat Institute.
I&rsquo;ve recently written together with Jules Hedges about Reinforcement Learning using the Categorical Cybernetics framework (you can find the ACT2024 paper here). Here I would like to talk about the main construction, which I like to refer to as the &ldquo;RL lens&rdquo;.
Selected aspects of RL In modelling disciplines, one often faces the challenge of balancing three often conflicting aspects: representational elegance, the breadth of examples to capture, and the depth or specificity in capturing those examples of interest. In the context of reinforcement learning theory, this raises the question: what is an adequate ontology for the techniques involved in agents learning from interaction with an environment?
"><meta name=keywords content="homepage,blog,Riu Rodriguez Sakamoto,Reinforcement Learning,Categorical Cybernetics"><meta name=robots content="noodp"><meta name=theme-color content><link rel=canonical href=https://riurodsak.github.io/posts/2024/05/rl_cat_cyb/><title>Reinforcement Learning through the lens of Categorical Cybernetics :: Riu Rodríguez Sakamoto
</title><link rel=stylesheet href=/main.949191c1dcc9c4a887997048b240354e47152016d821198f89448496ba42e491.css integrity="sha256-lJGRwdzJxKiHmXBIskA1TkcVIBbYIRmPiUSElrpC5JE="><link rel=stylesheet type=text/css href=https://riurodsak.github.io/css/custom.css><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color><link rel="shortcut icon" href=/favicon.ico><meta name=msapplication-TileColor content><meta itemprop=name content="Reinforcement Learning through the lens of Categorical Cybernetics"><meta itemprop=description content="Cross-posted from Cybercat Institute.
I’ve recently written together with Jules Hedges about Reinforcement Learning using the Categorical Cybernetics framework (you can find the ACT2024 paper here). Here I would like to talk about the main construction, which I like to refer to as the “RL lens”.
Selected aspects of RL In modelling disciplines, one often faces the challenge of balancing three often conflicting aspects: representational elegance, the breadth of examples to capture, and the depth or specificity in capturing those examples of interest. In the context of reinforcement learning theory, this raises the question: what is an adequate ontology for the techniques involved in agents learning from interaction with an environment?"><meta itemprop=datePublished content="2024-05-28T10:56:57+01:00"><meta itemprop=dateModified content="2024-05-28T10:56:57+01:00"><meta itemprop=wordCount content="1473"><meta itemprop=image content="https://riurodsak.github.io/posts/2024/05/rl_cat_cyb/thumbnail.png"><meta itemprop=keywords content="Reinforcement Learning,Categorical Cybernetics"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://riurodsak.github.io/posts/2024/05/rl_cat_cyb/thumbnail.png"><meta name=twitter:title content="Reinforcement Learning through the lens of Categorical Cybernetics"><meta name=twitter:description content="Cross-posted from Cybercat Institute.
I’ve recently written together with Jules Hedges about Reinforcement Learning using the Categorical Cybernetics framework (you can find the ACT2024 paper here). Here I would like to talk about the main construction, which I like to refer to as the “RL lens”.
Selected aspects of RL In modelling disciplines, one often faces the challenge of balancing three often conflicting aspects: representational elegance, the breadth of examples to capture, and the depth or specificity in capturing those examples of interest. In the context of reinforcement learning theory, this raises the question: what is an adequate ontology for the techniques involved in agents learning from interaction with an environment?"><meta property="article:published_time" content="2024-05-28 10:56:57 +0100 +0100"><!doctype html><html><head><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script defer>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1}]})})</script></head></html><link rel=stylesheet type=text/css href=/css/hugo-cite.css></head><body><div class=container><header class=header><span class=header__inner><a href=/ style=text-decoration:none><div class=logo><span class=logo__mark>></span>
<span class=logo__text>/home</span>
<span class=logo__cursor style=visibility:hidden></span></div></a><span class=header__right><nav class=menu><ul class=menu__inner><li><a href=/posts>Blog</a></li></ul></nav><span class=menu-trigger><svg viewBox="0 0 24 24"><path d="M0 0h24v24H0z" fill="none"/><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/></svg>
</span><span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none"><path d="M22 41c10.4934.0 19-8.5066 19-19C41 11.5066 32.4934 3 22 3 11.5066 3 3 11.5066 3 22s8.5066 19 19 19zM7 22C7 13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22z"/></svg></span></span></span></header><div class=content><main class=post><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>
7 minutes</p></div><article><h1 class=post-title><a href=https://riurodsak.github.io/posts/2024/05/rl_cat_cyb/>Reinforcement Learning through the lens of Categorical Cybernetics</a></h1><div class=post-content><p>Cross-posted from <a href=https://cybercat.institute/2024/05/29/reinforcement-learning-in-cat-cyb/>Cybercat Institute</a>.</p><p>I&rsquo;ve recently written together with <a href=https://julesh.com/>Jules Hedges</a> about Reinforcement Learning using the Categorical Cybernetics framework (you can find the ACT2024 paper <a href=https://arxiv.org/abs/2404.02688>here</a>).
Here I would like to talk about the main construction, which I like to refer to as the &ldquo;RL lens&rdquo;.</p><h1 id=selected-aspects-of-rl>Selected aspects of RL</h1><p>In modelling disciplines, one often faces the challenge of balancing three often conflicting aspects: representational elegance, the breadth of examples to capture, and the depth or specificity in capturing those examples of interest.
In the context of reinforcement learning theory, this raises the question: what is an adequate ontology for the techniques involved in agents learning from interaction with an environment?</p><p>Here we make a structural approach to the above dilemma, both in the sense of structural realism and <a href=https://ncatlab.org/nlab/show/stuff%2C+structure%2C+property>stuff, structure, property</a>.
The characteristics of RL algorithms that we capture are their modularity and specification via typed interfaces.</p><p>To keep this exposition grounded in something practical, we will follow an example, <a href=https://en.wikipedia.org/wiki/Q-learning>Q-learning</a>, which from this point of view captures the essence of reinforcement learning.
It is an algorithm that finds an optimal policy in a MDP by keeping an estimate of value of taking a certain action in a certain state, encoded as a table $Q:A\times S\to R$, and updating it from previous estimates (<em>bootstrapping</em>) and from samples obtained by interacting with an environment.
This is the content of the following equation (we&rsquo;ll give the precise type for it later):</p><p>$$ \begin{equation}
Q(s,a) \gets (1-\alpha) Q(s,a) + \alpha [r + \max_{a&rsquo;:A}Q(s&rsquo;,a&rsquo;) ]
\end{equation} $$</p><p>One does also have a policy that is derived from the $Q$ table, usually an $\varepsilon$-greedy policy that selects with probability $1-\varepsilon$ for a state $s$ the action that maximizes the estimated value, $\max_{a:A}Q(s,a)$, and a uniformly sampled action with probability $\varepsilon$.
This choice helps to overcome the exploration-exploitation balance.</p><p>Ablating either component produces other existing algorithms, which is reassuring:</p><ul><li>If we remove the bootstrapping component, one recovers a (model-free) one-step Monte Carlo algorithm.</li><li>If we remove the samples, one recovers classical Dynamic Programming methods<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> such as Value Iteration. We&rsquo;ll come back to these sample-free algorithms <a href=#continuations-function-spaces-and-lenses-in-lenses>later</a>.</li></ul><h1 id=the-rl-lens>The RL lens</h1><p>Q-learning as we&rsquo;ve just described, and other major RL algorithms, can be captured as lenses; the forward map is the policy deployment from the models parameters, and the backward map is the update function.</p><p><img src=generic_model.svg#svgstyle alt="Generic model lens"></p><p>The interface types vary from algorithm to algorithm.
In the case of Q-learning, the forward map $P$ is of type $R^{S\times A}\to (DA)^S$. It takes the current $Q$-table $Q:S\times A\to R$ and outputs a policy $S\to DA$. This is our $\varepsilon$-greedy policy defined earlier.
The backward map $G$ has the following type (we define $\tilde{Q}$ in (2)):
$$ \begin{align*}
R^{S\times A}\times (S\times A\times R\times S) &\to T_{(s,a)}^{*}(S\times A) \newline
Q, (s,a,r,s) &\mapsto \tilde{Q}
\end{align*} $$</p><p><img src=Q_learning_model.svg#svgstyle alt="Q-learning model lens"></p><p>The type of model parameter change $\Delta(R^{S\times A})=T_{(s,a)}^{*}(S\times A)$ has as elements cotangent vectors to the base space $S\times A$ (not to $R^{S\times A}$).
This technicality allows us to define the pointwise update of equation (1) as $((s,a),g)$, where $g=(r + \gamma\max_{a&rsquo;:A}Q(s&rsquo;,a&rsquo;))\in R$ is the <em>update target</em> of our model.
The new $Q$ function then is defined as:</p><p>$$ \begin{equation}
\tilde{Q}(\tilde{s},\tilde{a}) = \begin{cases}
(1-\alpha)Q(s,a) + \alpha [r + \gamma \max_{a&rsquo;} Q(s&rsquo;,a&rsquo;)] & (\tilde{s},\tilde{a})=(s,a) \newline
Q(s,a) & o/w
\end{cases} \end{equation} $$</p><p>The quotes in the diagram above reflects that showing explicitly the $S$ and $A$ wires below loses the dependency of the type $R^{S\times A}$ over them.
This is why in the paper we prefer to write the backward map as a single box $G$ with inputs $R^{S\times A}\times (S\times A\times R)$ and output $T_{(s,a)}^{*}(S\times A)$.</p><h1 id=from-q-learning-to-deep-q-networks>From Q-learning to Deep Q-networks</h1><p>Writing the change type as a cotangent space allows us to bridge the gap to Deep Learning methods.
In our running example, we can do the standard transformation of the Bellman update to a Bellman error to decompose $G$ into two consecutive steps:</p><ul><li><p>Backward map:</p><p>$$ \begin{align*}
G:R^{S\times A} \times (S\times A\times R\times S&rsquo;) &\to S\times A\times R \newline
Q, (s,a,r,s&rsquo;) &\mapsto (s,a,\mathcal{L})
\end{align*} $$</p><p>The loss $\mathcal{L}$ is defined as the MSE between the current $Q$-value and the update target $g$:
$$ \mathcal{L} = \left(Q(s,a) - g\right)^2 = \left(Q(s,a) - (r + \gamma \max_{a&rsquo;} \bar{Q}(s&rsquo;,a&rsquo;)) \right)^2 $$
We treat $\bar{Q}(s&rsquo;,a&rsquo;)$ ($Q$ bar) as a constant value, so that the (semi-)gradient of $\mathcal{L}$ wrt. the $Q$-matrix <em>is</em> the Bellman Q-update, as we show next.</p></li><li><p>Feedback unit (Bellman update):</p><p>$$ \begin{align*}
(1+S\times A\times R)\times R^{S\times A} \to& R^{S\times A} \newline
*, Q \mapsto& Q \newline
(s,a,\mathcal{L}), Q \mapsto& \tilde{Q} \newline
=& Q - {\alpha\over 2}{\partial\mathcal{L}\over\partial Q} \newline
=& \forall (\tilde{s},\tilde{a}). \begin{cases}
Q(s,a) - \alpha(Q(s,a) - g) & (\tilde{s},\tilde{a}) = (s,a) \newline
Q(s,a) & o/w
\end{cases} \newline
& \forall (\tilde{s},\tilde{a}).\begin{cases}
(1-\alpha) Q(s,a) + \alpha g & (\tilde{s},\tilde{a}) = (s,a) \newline
Q(s,a) & o/w
\end{cases}
\end{align*} $$</p><p>This recovers (2), so we can say that the backwards map is doing <em>pointwise</em> gradient descent, by only updating the $(s,a)$ indexed $Q$-value.</p></li></ul><h1 id=continuations-function-spaces-and-lenses-in-lenses>Continuations, function spaces, and lenses in lenses</h1><p>Focusing now on sample-free algorithms, the model&rsquo;s parameter update is an operator $(X\to R)\to (X\to R)$ between function spaces.
State value methods for example update value functions $S\to R$, whereas state-action value methods update functions $S\times A\to R$ (the $Q$-functions).
More concretely, the updates of function spaces that appear in RL are known as Bellman operators.
It turns out that a certain subclass which we call <em>linear Bellman operators</em> can be obtained functorially from lenses as well!</p><p>The idea is to employ the continuation functor<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> which is the following representable functor:</p><p>$$ \begin{align*}
\mathbb{K} =\mathbf{Lens}(-,1) : \mathbf{Lens}^\mathrm{op} &\to \mathbf{Set} \newline
{X\choose R} &\mapsto R^X \newline
{X\choose R}\rightleftarrows {X&rsquo;\choose R&rsquo;} &\mapsto R&rsquo;^{X&rsquo;} \to R^X
\end{align*} $$</p><p>The contravariance hints already at the corecursive nature of these operators:
They take as input a value function of states <em>in the future</em>, and return a value function of states <em>in the present</em>.
The subclass of Bellman operators that we obtain this way is linear in the sense that it uses the domain function in $R&rsquo;^{X&rsquo;}$ only once.</p><p>An example of this is the value improvement operator from dynamic programming.
This operator improves the value function $V:S\to R$ to give a better approximation of the long-term value of a policy $\pi:S\to A$, and is given by</p><p>$$ V(s) \gets \mathbb{E}_{\mkern-14mu\substack{a\sim \pi(s)\newline (s&rsquo;,r)\sim t(s,a)}}[r+\gamma V(s&rsquo;)] = \sum _{a\in A}\pi(a\mid s) \sum _{\substack{s&rsquo;\in S\newline r\in R}}t(s&rsquo;,r\mid s, a) (r + \gamma V(s&rsquo;)) $$</p><p>This is the image under $\mathbb{K}$ of a lens <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> whose forward and backward maps are the transition function $\mathrm{pr}_1(t(-,\pi(-))):S \to S$ under a fixed policy $\pi:S\to A$, and the update target computation $(-)+\gamma\cdot(=):\R\times \R\to \R$ respectively, as shown below.</p><p><img src=VI_lens_to_set.svg#svgstyle alt="Value Improvement lens to function"></p><p>If you want to read more about this &ldquo;optics perspective&rdquo; on Value Iteration and its relation with problems like the control of an inverted pendulum, the savings problem in economics and more, check out our previous <a href=https://arxiv.org/abs/2206.04547>ACT2022 paper</a>.</p><p>Once we have transformed the Bellman operator into a function using $\mathbb{K}$, this embeds into the backward map of the RL lens.</p><p><img src=Bellman_embedding_into_lens.svg#svgstyle alt="Embedding of the Bellman operator into the backward pass of the RL lens"></p><p>It is then natural to ask what a backward map that does not ignore the sample input might look like, and these are what we call <em>parametrised</em> Bellman operators.
These are obtained by the lifting of $\mathbb{K}$ to the (externally parametrised) functor $\mathrm{Para}(\mathbb{K}):\mathrm{Para}(\mathrm{Lens}^\mathrm{op})\to\mathrm{Set}$, and captures exactly what algorithms like <a href=https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action>SARSA</a> are doing in terms of usage of both bootstrapping and sampling.</p><h1 id=outlook>Outlook</h1><p>We talked about learning from bootstrapping and from sampling as two distinct processes that fit into the lens structure. While the difference between these two is usually not emphasized enough, we believe that it is useful for understanding the structure of novel algorithms by making the information flow explicit.
You can find more details, along with a discussion on Bellman operators, the <a href=https://cybercat.institute/2024/02/22/iteration-optics/>iteration functor</a> used to model stateful environments, prediction and bandit problems as nice corner cases of our framework, and more on our recent <a href=https://arxiv.org/abs/2404.02688>submission</a> to ACT2024.</p><p>Moreover, this opens up the study of stateful models: multi-step methods like $n$-temporal difference or Monte Carlo Tree Search (used e.g. in AlphaZero), which we will leave for a future post, so stay tuned!</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This is sometimes denoted &ldquo;offline&rdquo; RL, but one should note that offline methods include learning from a constant dataset and learning by updating one&rsquo;s estimates only at the end of episodes too.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>In general, the continuation functor is defined for any optic as $\mathbb{K}=\mathbf{Optic}(\mathcal{C})(-,I):\mathbf{Optic}(\mathcal{C})^\mathrm{op}\to\mathbf{Set}$, represented by the monoidal unit $I$.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Ok I lied here a little: To be precise, the equation shown arises as the continuation of a <em>mixed</em> optic, where the forwards category is $\mathrm{Kl}(D)$ for a probability monad $D$, and the backwards category is $\mathrm{EM}(D)$. The value improvement operator that arises from the continuation of a lens is a deterministic version of this, where there&rsquo;s no expectation taken in the backwards pass because we fix the policy and the transition functions to be deterministic.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><hr><div class=post-info><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7" y2="7"/></svg>
<span class=tag><a href=https://riurodsak.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></span>
<span class=tag><a href=https://riurodsak.github.io/tags/categorical-cybernetics/>Categorical Cybernetics</a></span></p><p><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>
2024-05-28 09:56</p></div><div class=pagination><div class=pagination__buttons><span class="button previous"><a href=https://riurodsak.github.io/posts/2024/10/concept-selection-in-systems-engineering-as-lenses-in-poly/><span class=button__icon>←</span>
<span class=button__text>Concept selection in systems engineering as lenses in Poly</span></a></span></div></div></main></div><footer class=footer><div class=footer__inner><div class=footer__content><span><a href=https://riurodsak.github.io/posts/index.xml target=_blank title=rss><svg width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></span></div></div><div class=footer__inner><div class=footer__content><span>Powered by <a href=http://gohugo.io>Hugo</a></span></div></div></footer></div><script type=text/javascript src=/bundle.min.e89fda0f29b95d33f6f4224dd9e5cf69d84aff3818be2b0d73e731689cc374261b016d17d46f8381962fb4a1577ba3017b1f23509d894f6e66431f988c00889e.js integrity="sha512-6J/aDym5XTP29CJN2eXPadhK/zgYvisNc+cxaJzDdCYbAW0X1G+DgZYvtKFXe6MBex8jUJ2JT25mQx+YjACIng=="></script></body></html>